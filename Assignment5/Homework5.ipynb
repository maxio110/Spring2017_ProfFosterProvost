{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science for Business Analytics\n",
    "## Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: \n",
    "\n",
    "Student Netid: \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Critique this plan\n",
    "1\\. After a few beers your CIO invited his buddy from Blue Moon consulting to propose a project using data mining to improve the targeting of the new service that you have been a principal in developing. The service has been quite successful so far, being marketed over the last 6 months via your ingenious, and very inexpensive, word-of-mouth campaign. You've already garnered a pretty large customer base without any targeting, and you've been seeing this success as your best stepping stone to bigger and better things in the firm. \n",
    "\n",
    "After some reflection, you've decided that your best course of action is to play a key role in ensuring the success of this data mining project as well. You agree with your CIO's statement in a meeting with Blue Moon, that accurate targeting might cost-effectively expand your audience substantially to consumers that word-of-mouth would not reach. You accept that what Blue Moon says about the characteristics of your service is accurate.\n",
    "\n",
    "Based on what we have covered in class and in the book, identify the four most serious weaknesses/flaws in this abridged version of Blue Moon's proposal, and suggest how to ameliorate them.  Your answer should be 4 bullet points, each comprising 2-4 sentences: 1-2 sentences stating each weakness, and 1-2 sentences suggesting a better alternative.  Maximal credit will be given when the 4 points are as independent as possible.\n",
    "\n",
    "\n",
    "```\n",
    "--- -------------------------------------------------------------------------- ---\n",
    "                            Targeted Audience Expansion             \n",
    "                      Prepared by Blue Moon Consulting, Inc.\n",
    "\n",
    "Your problem is to expand the audience of your new service.  We (Blue Moon) have a \n",
    "large database of consumers who can be targeted.  We will build a predictive model \n",
    "to estimate which of these consumers are the most likely to adopt the product, and\n",
    "then target them with the special offer you have designed.\n",
    "\n",
    "More specifically, we will build a logistic regression (LR) model to predict adop-\n",
    "tion of the service by a consumer, based on the data on your current customers of \n",
    "this service.  The model will be based on their demographics and their usage of \n",
    "the service. We believe that logistic regression is the best choice of method be-\n",
    "cause it is a tried-and-true statistical modeling technique, and we can easily \n",
    "interpret the coefficients of the model to infer whether the attributes are stat-\n",
    "istically significant, and whether they make sense. If they are statistically sig-\n",
    "nificant and they do make sense, then we can have confidence that the model will \n",
    "be accurate in predicting service uptake. We will apply the model to our large \n",
    "database of consumers, and select out those who have not yet subscribed and whom\n",
    "the LR model predicts to be the most likely to subscribe.  To these we will send \n",
    "the targeted offer. As this is a fixed-profit-per-customer service, this also \n",
    "will in effect rank them by expected profit.\n",
    "--- -------------------------------------------------------------------------- ---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Profit\n",
    "1\\. **Advertising cost/benefit matrix**: Imagine that you work for an online advertising company that has just been hired to advertise a new local restaurant online. Let's say that it costs \\$0.015 to present a coupon ad to online consumers. If a consumer cashes in your coupon, you stand to earn \\$5. Given this information, what would your cost/benefit matrix be?  Explain your reasoning briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cost_matrix_ads = \n",
    "\n",
    "\n",
    "\n",
    "# Don't delete this line\n",
    "print (cost_matrix_ads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. **Churn cost/benefit matrix** Despite all your retention efforts, customers still are leaving your company, and when a customer leaves the company incurs various costs.   The company has created a cost-reduction plan (CRP) for those who **do** churn, separate from the ongoing retention efforts.  The CRP will be targeted to those most likely to churn, based on a model you build.  If the plan is given to someone who otherwise would churn, the company's cost will be reduced by \\$15.  The cost of enacting the plan for a customer is \\$2.  What would your cost matrix be in this case?  Give a short explanation for your reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "cost_matrix_churn = \n",
    "\n",
    "\n",
    "# Don't delete this line\n",
    "print (cost_matrix_churn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. You know that the Bernoulli naive Bayes classifier works by calculating the conditional probabilities of each feature, $e_i$, occuring with each class $c$ and treating them independently. This results in the probability of a certain class occuring given a set of features, or a piece of evidence, $E$, as\n",
    "\n",
    "$$P(c \\mid E) = \\frac{p(e_1 \\mid c) \\cdot p(e_2 \\mid c) \\cdot \\cdot \\cdot p(e_k \\mid c) \\cdot p(c)}{p(E)}.$$\n",
    "\n",
    "The conditional probability of each piece of evidence occuring for a given class is computed from the data as:\n",
    "\n",
    "$$P(e_i \\mid c) = \\frac{\\text{count}(e_i, c)}{\\text{count}(c)}.$$\n",
    "\n",
    "In the above equation $\\text{count}(e_i, c)$ is the number of instances (say, documents) in a given class that contain feature $e_i$ and $\\text{count}(c)$ is the number of instances that belong to class $c$. \n",
    "\n",
    "A common variation of the above is to use Laplace (sometimes called +1) smoothing. This is done in sklearn by setting `alpha=1` in the `BernoulliNB()` function (this is also the default behavior). The result of Laplace smoothing will slightly change the conditional probabilities,\n",
    "\n",
    "$$P(e_i \\mid c) = \\frac{\\text{count}(e_i, c) + 1}{\\text{count}(c) + 2}.$$\n",
    "\n",
    "In no more than **one paragraph**, describe why this is useful. Come up with a scenario where not using Laplace smoothing would result in a \"bad\" model and give an example. Be precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer here!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Text features\n",
    "For this part of the assignment, we are going to build a sentiment analysis model.  It will estimate the sentiment of movie reviews from IMDB.com. The data consists of the text of a movie review and a target variable which tells us whether the reviewer had a positive feeling towards the movie (equivalent to rating the movie between 7 and 10) or a negative feeling (rating the movie between 1 and 4). Neutral reactions are not included in the data.\n",
    "\n",
    "The data are located in \"`data/imdb.csv`\". The first column is the review text; the second is the text label 'P' for positive or 'N' for negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Load the data into a pandas `DataFrame()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Code the target variable to be numeric: use the value `1` to represent 'P' and `0` to represent 'N'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Put all of the text into a data frame called `X` and the target variable in a data frame called `Y`. Make a train/test split where you give 75% of the data to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Create a binary `CountVectorizer()` and a `TfidfVectorizer()`. Use the original single words as well as bigrams. Fit these to the training data to extract a vocabulary and then transform both the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Create `LogisticRegression()` and `BernoulliNB()` models. For all settings, keep the default values. In a single plot, show the AUC curves for both classifiers; for Logistic Regression try both the binary and tfidf feature sets (3 curves). In the legend, include the area under the ROC curve (AUC). Do not forget to label your axes. Your final plot will be a single window with 4 curves.\n",
    "\n",
    "Which model do you think does a better job? Why? Explain in no more than a paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this so your plots show properly\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 12, 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Now, create a binary `CountVectorizer()` and `TfidfVectorizer()` but use the \"english\" stop word list this time (that's a particular stoplist in sklearn). Use the original single words as well as bigrams. Fit the vectorizors with the training set. Then transform both: train and test. \n",
    "\n",
    "Then, using the new vectorizers, create the same plot as in the last question:  \"LogisticRegression() and BernoulliNB() models. For all settings, keep the default values. In a single plot, show the AUC curve for both classifiers and both the binary and tfidf feature sets. In the legend, include the area under the ROC curve (AUC). Do not forget to label your axes. **Your final plot will be a single window with 4 curves.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Improve the best Logistic regression and the best NB found in question 6. Try different values for 'C' (complexity) and different alphas for NB. \n",
    "\n",
    "** Plot your results !!!! **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "8\\. Use the model that you think did the best job and predict the rating of the test data. Find 5 examples that were in fact positive, but **which were incorrectly scored highly (classified) as negative.** List the text below and include an explanation as to why you think it may have been incorrectly estimated. You can pick any 5. They do not have to be at random. (Use the last vectorizers created, those including stop words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Explanation for the 5 reviews chosen here!_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9a\\. Let's create a profit curve. **First we need a cost/benefit matrix.....**\n",
    "\n",
    "This might be slightly contrived, but let's say that we want to show \"positive\" reviews of movies to users, to entice them to stream them.  We've done a study that shows that showing a positive review in expectation yields us \\$7, but incorrectly showing a negative review (as a positive) costs us \\$15 in reduced customer satisfaction.  We've decided that we don't want to show the same review twice, so as we need to show more reviews, we will go down our list ranked by predicted positive sentiment.  We can always stop showing reviews at all if we've gotten too far down the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "cost_matrix_movies =\n",
    "\n",
    "\n",
    "# Don't delete this line\n",
    "print (cost_matrix_movies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9b\\. Let's create a profit curve. Using the cost matrix for these movies and plot the profit curve of the previous 4 models: `LogisticRegression()` and `BernoulliNB()` with binary and tfidf features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Explain what you've found from the profit curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
