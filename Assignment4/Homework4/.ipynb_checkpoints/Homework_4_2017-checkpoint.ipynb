{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science\n",
    "## Homework 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: \n",
    "\n",
    "Student Netid: \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Basic evaluations\n",
    "For this part of the assignment, we are going to use another (different) churn data set, located in `data/churn.csv`. The first 11 columns are predictor variables.\n",
    "\n",
    "```\n",
    "COLLEGE                       college educated?\n",
    "INCOME                        income\n",
    "OVERAGE                       average overcharges per month\n",
    "LEFTOVER                      average % leftover minutes per month\n",
    "HOUSE                         value of dwelling (from census tract)\n",
    "HANDSET_PRICE                 cost of phone\n",
    "OVER_15MINS_CALLS_PER_MONTH   average number of long calls per month\n",
    "AVERAGE_CALL_DURATION         average call duration\n",
    "REPORTED_SATISFACTION         reported level of satisfaction\n",
    "REPORTED_USAGE_LEVEL          self-reported usage level\n",
    "CONSIDERING_CHANGE_OF_PLAN    was customer considering changing plan?\n",
    "```\n",
    "\n",
    "The last column, `LEAVE`, is the target variable that equals one if the user left and zero if they stayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Load the data into a pandas `DataFrame()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Some of the columns are categorical variables. You have to turn these into zero/one **\"dummy\" variables**.  \n",
    "\n",
    "The idea is that for modeling methods like logistic regression which take only numeric inputs, we should change categorical variables. Basically, we must create one (new) **binary variable** for each value of the categorical. Technically if there are k values, we really need only to create new variables for k-1 of the categories, but often we create k new variables with all categories anyway, as that makes the model more interpretable.\n",
    "\n",
    "The following code is to help you understanding dummy variables, you might need to change it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for field in ['REPORTED_SATISFACTION', 'REPORTED_USAGE_LEVEL','CONSIDERING_CHANGE_OF_PLAN']:\n",
    "    \n",
    "    #Take all UNIQUE categories from each column \n",
    "    \n",
    "    for value in data[field].unique():\n",
    "    \n",
    "    \n",
    "        # In the DataFrame called \"data\", create a new column with the original name + the category name\n",
    "        # This new column will be created by \"False\" and \"True\" for each unique value in the field.\n",
    "        # Then, those True and False values can be transformed into  \"integer\" (e.g. 1 and 0)\n",
    "        \n",
    "        data[field + \"_\" + value] = pd.Series(data[field] == value,dtype=int)\n",
    "\n",
    "        \n",
    "    # Drop the original field (column), we only want to keep the new \"dummy variables\"\n",
    "    # Look at the them at the end of the data frame that will be printed after running this cell\n",
    "    \n",
    "    data = data.drop([field], axis=1)\n",
    "    \n",
    "    \n",
    "data.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Put all of the predictors into a data frame called `X` and the target variable in a data frame called `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Create `LogisticRegression()` and `DecisionTreeClassifier()` models. For the tree-structured model, use `criterion=\"entropy\"` and `min_samples_split=50` (the latter to make sure the tree is not making decisions from too few data). For all other settings, keep the default values. \n",
    "\n",
    "For these models, obtain the accuracy estimates (average) and standard deviations based on cross-validation with different fold values: 2, 5, 10, 20, 50, and 75. \n",
    "Store your results in **lr_all_acc**, **dt_all_acc**, **lr_all_std**, **dt_all_std**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "cv_number_folds = [2,5,10,20,50,75]\n",
    "\n",
    "# Logistic regression cross-validation accuracies\n",
    "lr_all_acc = []   \n",
    "lr_all_std = []\n",
    "# Decision tree cross-validation accuracies\n",
    "dt_all_acc = []\n",
    "dt_all_std = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# These lines will be used for GRADING. Ensure that they print what they should.\n",
    "\n",
    "print ( \"Logistic regression folds: Mean Accuracy %.4f\" % (np.mean(lr_all_acc)))\n",
    "print (\"Tree folds: Mean Accuracy %.4f\" % (np.mean(dt_all_acc)) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Now **plot** your results: for each modeling method, plot the average generalization performance with +/- one standard deviation error bars.  **Explain** what the graph reveals. ** Explain:** which number of folds would you select and why? \n",
    "\n",
    "Here is how the plots should look like  \n",
    "\n",
    "_Hint:  plt.errorbar might be helpful_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 10, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 3 artists>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmMAAAD/CAYAAAC989FsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1KH/HzTrS6kwyKoQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d668990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "## CODE FOR YOUR GRAPHS HERE: \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answers here! **\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Create the same **plots** for a different evaluation metric: Area under the ROC curve (AUC score).  (**Read Chapter 8**) \n",
    "\n",
    "**Do you draw the same conclusions?**  Which metric would you prefer for this problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_number_folds = [2,5,10,20,50,75]\n",
    "\n",
    "# Logistic regression cross-validation auc\n",
    "lr_all_auc = []   \n",
    "lr_all_std = []\n",
    "# Decision tree cross-validation auc\n",
    "dt_all_auc = []\n",
    "dt_all_std = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer here! **\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Find the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Create `LogisticRegression()` and `DecisionTreeClassifier()` models. For the logistic regression **find the optimal value** for regularization (`C`) using 10-fold cross validation and area under the ROC curve (AUC). Use the same procedure to find the optimal value of `min_samples_split` for the decision tree classifier (criterion='entropy'). For each of these values, try 20 different choices. To show your results, **create two plots**, one for each modeling method, that show the value of AUC on the y-axis and the parameter values you tried on the x-axis. **Don't forget to label your axes!**  \n",
    "\n",
    "[Hint 1: for a coarse search of the space of complexity parameters a good heuristic is to cover the range with an exponentially increasing set of values.  We're already doing that with C.  How could you get a good coverage of the range of possible min_samples_split?] \n",
    "\n",
    "[Hint 2: Plotting with a log scale on the x-axis sometimes helps. Note we're already doing that by plotting C.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Code to test and create a plot for LogisticRegression() here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Code to test and create a plot for DecisionTreeClassifier() here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Split your data to create different training sets with size 10%, 20%, 40%, 80%, and 90% of the size of the original data set. Fit two models for the different training sets: decision tree (entropy criteria) and logistic regression. Use the **best parameters** from the previous question and 10-fold cross validation. Plot the **learning curves** for the two models on one plot, including +/- one standard deviation error bars and using auc score.  Use log scale on the x-axis (hint: using basex=2 looks much nicer than the default). Make sure your plot has a legend so that you can tell which curve is related to which model. **Don't forget to label your axes **.  \n",
    "\n",
    "**Explain what the learning curves reveal**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "sample_sizes = [.1, .2, .4, .8, .9]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Find the best model: **GRID SEARCH**\n",
    "\n",
    "Ok. So sklearn provides a function that searches for the best modeling paramenters, as you did above.  This is very much \"machine learning style\" modeling, where as much as possible is inferred from the data--in particular, even the right values for the modeling parameters.\n",
    "\n",
    "1\\. Use the sklearn function GridSearchCV to perform a grid search. This is a search through the modeling parameters that we have been using before (e.g. regularization \"c\", min_samples_split, ...). They are called \"hyper-parameters\" and we want to find the best coombination of them for our model (not individually like we did it before).   \n",
    "\n",
    "This grid-search function uses a Python **dictionary** to lay out the values that we want to search among. \n",
    "\n",
    "Then, using the same lists of values that you used above (e.g. all the min_samples_split of Part2, question1), create a dictionary and use it for the grid search.\n",
    "\n",
    "Example: GridSearchCV( DecisionTreeClassifier(), parameters_dictionary )\n",
    "\n",
    "[You can find the documentation here](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Which are the best values for regularization and min_samples_split with this function? Did they change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Evaluate the grid search again but this time include in the dictionary for logistic regression a list with penalty 'l1' and 'l2'. Also, for the decision tree, include a list of values for \"min_samples_leaf= [100, 200, 300, 400, 500, 1000]\". \n",
    "\n",
    "**What is the best selection for logistic regression and decision tree now?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ok.  One last thing.  In the above analysis, although we found a set of best parameters, we chose them from performance over many cross-validations.  Thus, we made modeling decisions using the test sets.  That's fine for choosing the hyperparameters, but compromises our estimates of the final accuracy (which you did not report).  Technically, if we want good estimates of the accuracy of the resultant models, we should do our evaluation on a test set that was not used to make *any* modeling decisions.  The book discusses \"nested cross-validation\" as a technique to accomplish this.  This is not required for the homework, but if you want to challenge yourself to do what a top-notch data scientist would do, use nested cross-validation to do the grid search for the best parameters *using data only from the training folds*, and then build the model for the test folds with the chosen parameter.  Or, alternatively, hold out a final test set at the beginning for the final testing.*  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No answer required."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
